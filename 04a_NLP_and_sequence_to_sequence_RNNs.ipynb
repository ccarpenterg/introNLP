{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04a_NLP_and_sequence-to-sequence_RNNs.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbiVg6WCVXY2vWSsw3C2Gn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/introNLP/blob/master/04a_NLP_and_sequence_to_sequence_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12TvrvBulJba",
        "colab_type": "text"
      },
      "source": [
        "# NLP and Sequence-to-Sequence RNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7C_h_GLDTbK",
        "colab_type": "code",
        "outputId": "7715a8df-2ab1-4b5a-868c-db1270af4df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "!wget https://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-18 00:04:44--  https://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:3037::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4767708 (4.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   4.55M  1.72MB/s    in 2.6s    \n",
            "\n",
            "2020-02-18 00:04:47 (1.72 MB/s) - ‘spa-eng.zip’ saved [4767708/4767708]\n",
            "\n",
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejo5g5kBECxP",
        "colab_type": "code",
        "outputId": "652c6d65-a691-4cd2-bae7-cc435180921e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset\n",
        "from torchtext import data\n",
        "\n",
        "import spacy\n",
        "\n",
        "from io import open\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "print(\"Spacy version:\", spacy.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spacy version: 2.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8wUoQ65g2u6",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the Natural Language Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAOScg5lgthG",
        "colab_type": "text"
      },
      "source": [
        "### Text Normalization functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64vE5FSE2tgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        char for char in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(char) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPudOEURc7Zl",
        "colab_type": "text"
      },
      "source": [
        "### Read Pairs function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_kg0w9K5XHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readPairs(pathToFile, slang, tlang):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    f = open(pathToFile, encoding='utf-8')\n",
        "    lines = f.read().strip().split('\\n')\n",
        "\n",
        "    pairs = [[normalizeString(s) for s in line.split('\\t')[:2]] for line in lines]\n",
        "\n",
        "    return pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-uO-hSZc2ao",
        "colab_type": "text"
      },
      "source": [
        "### Filter function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV0nAsYq8ClC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am\", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[0].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYAokqACdAxo",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Data function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRDq95CW_ll8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(pathToFile, slang, tlang):\n",
        "    pairs = readPairs(pathToFile, slang, tlang)\n",
        "    print(\"Read {} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {} sentence pairs\".format(len(pairs)))\n",
        "    return pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX7QHkDddJiq",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Indexes (Splits) function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFs44kLn_INN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def datasetIndexes(pairs, train=0.8, val=0.1):\n",
        "    num_examples = len(pairs)\n",
        "    indexes = list(range(num_examples))\n",
        "    last_train_idx = round(train*num_examples)\n",
        "    last_valid_idx = last_train_idx + round(val*num_examples)\n",
        "    random.shuffle(indexes)\n",
        "    train_idxs = indexes[:last_train_idx]\n",
        "    val_idxs = indexes[last_train_idx:last_valid_idx]\n",
        "    test_idxs = indexes[last_valid_idx:]\n",
        "    return train_idxs, val_idxs, test_idxs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTKk0Hc_dedI",
        "colab_type": "text"
      },
      "source": [
        "### Spanish-English Translation Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H19pHY6QPyF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpanishEnglishDataset(TranslationDataset):\n",
        "    \"\"\"English to Spanish Dataset\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, exts, fields, root='.data/',\n",
        "               train='train', validation='val', test='test', **kwargs):\n",
        "        \n",
        "        if 'path' not in kwargs:\n",
        "            expected_folder = os.path.join(root, cls.name)\n",
        "            path = expected_folder if os.path.exists(expected_folder) else None\n",
        "        else:\n",
        "            path = kwargs['path']\n",
        "            del kwargs['path']\n",
        "        \n",
        "        return super(SpanishEnglishDataset, cls).splits(\n",
        "            exts, fields, path, root, train, validation, test, **kwargs\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QisIUx_5drmV",
        "colab_type": "text"
      },
      "source": [
        "## Spanish-English Dataset files (splits)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKp8BQP6DX6c",
        "colab_type": "code",
        "outputId": "a8d767fc-3da6-429e-e637-a2d29dd8ea88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "pairs = prepareData('spa.txt', 'eng', 'spa')\n",
        "train_idxs, val_idxs, test_idxs = datasetIndexes(pairs)\n",
        "print(\"{} {} {}\".format(len(train_idxs), len(val_idxs), len(test_idxs) ))\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 123335 sentence pairs\n",
            "Trimmed to 7588 sentence pairs\n",
            "6070 759 759\n",
            "['he s not in our group .', 'no esta en nuestro grupo .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf-RZx3YGSDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('train.en', 'w') as slang_file, open('train.es', 'w') as tlang_file:\n",
        "    for i in train_idxs:\n",
        "        slang_file.write(pairs[i][0] + '\\n')\n",
        "        tlang_file.write(pairs[i][1] + '\\n')\n",
        "\n",
        "with open('val.en', 'w') as slang_file, open('val.es', 'w') as tlang_file:\n",
        "    for i in val_idxs:\n",
        "        slang_file.write(pairs[i][0] + '\\n')\n",
        "        tlang_file.write(pairs[i][1] + '\\n')\n",
        "\n",
        "with open('test.en', 'w') as slang_file, open('test.es', 'w') as tlang_file:\n",
        "    for i in test_idxs:\n",
        "        slang_file.write(pairs[i][0] + '\\n')\n",
        "        tlang_file.write(pairs[i][1] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SPfenbVeFlS",
        "colab_type": "text"
      },
      "source": [
        "## Loading the files (train, val, test) and Creating a SpanishDatatset instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jBTDFm1jgS8A",
        "outputId": "3c8cb513-6d1e-4015-d1a4-32afd8abacf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download es"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting es_core_news_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.1.0/es_core_news_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 830kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.1.0-cp36-none-any.whl size=11111556 sha256=610c0bef86d106381f225c06f41b9a006e0b25795a1a5e6d8fbd39c6894fed08\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kpyvxn_h/wheels/cc/ee/c4/68922955901918a9aaa82e828d4f7ee1ccfc861285277e79b7\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/es_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/es\n",
            "You can now load the model via spacy.load('es')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y03SbEQ1X54e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_es = spacy.load('es')\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenize_es(text):\n",
        "    \"\"\"\n",
        "    Tokenizes Spanish text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [token.text for token in spacy_es.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [token.text for token in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-37Cb1XYY-08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = data.Field(tokenize=tokenize_en, init_token='<SOS>', eos_token='<EOS>', lower=True)\n",
        "TRG = data.Field(tokenize=tokenize_es, init_token='<SOS>', eos_token='<EOS>', lower=True)\n",
        "\n",
        "train_data, valid_data, test_data = SpanishEnglishDataset.splits(\n",
        "    path='',\n",
        "    exts=('.en', '.es'),\n",
        "    fields=(SRC, TRG)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYPDAg7YiSu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq=5)\n",
        "TRG.build_vocab(train_data, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVGpH1PiaLpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QIyVoSKcIfH",
        "colab_type": "code",
        "outputId": "1ebbbc6b-28c7-4f14-883d-2d1405a6443b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(len(test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6070\n",
            "759\n",
            "759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRCx6WAEeg5P",
        "colab_type": "text"
      },
      "source": [
        "## Vocabulary and Training Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlMdtj01WXIU",
        "colab_type": "code",
        "outputId": "cdf97f54-8911-476e-c898-3fa4348f6e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "SRC.vocab.itos[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', '<SOS>', '<EOS>', '.', 'i', 'm', 're', 'you', 'he']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7DsRjXshe0B",
        "colab_type": "code",
        "outputId": "e8466ee3-df1e-4f6c-ae73-b3817ca4809d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "batch = next(iter(train_iterator))\n",
        "\n",
        "src_example = batch.src[:, 0]\n",
        "src = ' '.join(map(lambda i: SRC.vocab.itos[i], src_example))\n",
        "trg_example = batch.trg[:, 0]\n",
        "trg = ' '.join(map(lambda i: TRG.vocab.itos[i], trg_example))\n",
        "\n",
        "print(\"English sentence:\", src)\n",
        "print(\"Spanish sentence:\", trg)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English sentence: <SOS> they are <unk> <unk> . <EOS> <pad> <pad> <pad> <pad>\n",
            "Spanish sentence: <SOS> ellos estan <unk> <unk> . <EOS> <pad> <pad> <pad> <pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp2GwmBkcOsm",
        "colab_type": "code",
        "outputId": "acddafbb-85c0-4475-ea54-8529b0bb8998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "SRC.vocab.freqs.most_common(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 5988),\n",
              " ('i', 2798),\n",
              " ('m', 2185),\n",
              " ('re', 1434),\n",
              " ('you', 1350),\n",
              " ('he', 1083),\n",
              " ('is', 967),\n",
              " ('to', 934),\n",
              " ('a', 933),\n",
              " ('s', 670)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJMj5_jRcdte",
        "colab_type": "code",
        "outputId": "458b460d-ca87-4c5d-dd4e-eee91696de49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "TRG.vocab.freqs.most_common(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 5976),\n",
              " ('estoy', 1279),\n",
              " ('el', 1105),\n",
              " ('de', 944),\n",
              " ('es', 908),\n",
              " ('no', 778),\n",
              " ('a', 700),\n",
              " ('soy', 574),\n",
              " ('un', 567),\n",
              " ('esta', 540)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wz-vwqTiVZS",
        "colab_type": "text"
      },
      "source": [
        "## Sequence-to-Sequence (Seq2Seq) Model (without Attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIw5655Ln_pT",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdvU822UiiY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim,\n",
        "                 hidden_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                           num_layers, dropout=dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src_sequences):\n",
        "\n",
        "        # src_sequences: (max sequences length, batch size)\n",
        "\n",
        "        seq_embeddings = self.dropout(self.embedding(src_sequences))\n",
        "\n",
        "        # src_seq_embeddings: (max sequences length, batch size, embedding dimension)\n",
        "\n",
        "        seq_hidden, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        # hidden = (num layers * num directions, batch size, hidden dim)\n",
        "        # cell = (num layers * num directions, batch size, hidden dim)\n",
        "\n",
        "        return hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xCwi9fDn6g2",
        "colab_type": "text"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mnj77LceFBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embedding_dim,\n",
        "                 hidden_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_di = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                           num_layers, dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, seq_at_t, hidden, cell):\n",
        "\n",
        "        # seq_at_t: (batch size) -> vector of sequences at time t\n",
        "\n",
        "        seq_at_t = seq_at_t.unsqueeze(0)\n",
        "\n",
        "        # seq_at_t: (1, batch size)\n",
        "\n",
        "        seq_at_t_embedded = self.dropout(self.embedding(seq_at_t))\n",
        "\n",
        "        # seq_at_t_embedded: (1, batch size, embedding dim)\n",
        "        # hidden = (num layers * num directions, batch size, hidden dim)\n",
        "        # cell = (num layers * num directions, batch size, hidden dim)\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(seq_at_t_embedded, (hidden, cell))\n",
        "\n",
        "        # num directions = 1\n",
        "        # output: (1, batch size, hidden dim)\n",
        "\n",
        "        pred_scores = self.fc(output.squeeze(0))\n",
        "\n",
        "        # pred_scores: (batch size, output dim)\n",
        "        # hidden = (num layers , batch size, hidden dim)\n",
        "        # cell = (num layers , batch size, hidden dim)\n",
        "\n",
        "        return pred_scores, hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUF3H8jGv-XF",
        "colab_type": "text"
      },
      "source": [
        "### Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJDsq0ruwEtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "\n",
        "        # src: (max src length, batch size)\n",
        "        # trg: (max trg length, batch size)\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # outputs: tensor to store the decoder output\n",
        "        # outputs: (target max length, batch size, target vocab size)\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # we'll plug the last hidden states of the encoder in the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # the first target sequence token: <sos>\n",
        "        seq_at_t = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            pred_scores, hidden, cell = self.decoder(seq_at_t, hidden, cell)\n",
        "\n",
        "            outputs[t] = pred_scores\n",
        "\n",
        "            # teacher_force: Boolean\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # argmax_token: Token\n",
        "            argmax_token = pred_scores.argmax(1)\n",
        "\n",
        "            # depending on teacher_forcing value, use the next token or argmax_token\n",
        "            seq_at_t = trg[t] if teacher_force else argmax_token\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glalL9ooiv0Y",
        "colab_type": "text"
      },
      "source": [
        "### Model: 2 LSTM layers, no pre-trained Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWjdBpS2jRkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab) # output_dim is both the output dim and target vocab size\n",
        "ENCODER_EMB_DIM = 256 # length of the encoder's embedding vectors\n",
        "DECODER_EMB_DIM = 256 # length of the decoder's embedding vectors\n",
        "HIDDEN_DIM = 512 # RNN's hidden units\n",
        "NUM_LAYERS = 2 # number of stacked LSTM layers\n",
        "ENCODER_DROPOUT = 0.5 # encoder's dropout probability\n",
        "DECODER_DROPOUT = 0.5 # decoder's dropout probability\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, ENCODER_EMB_DIM, HIDDEN_DIM, NUM_LAYERS, ENCODER_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DECODER_EMB_DIM, HIDDEN_DIM, NUM_LAYERS, DECODER_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVO5JJfAmq4x",
        "colab_type": "code",
        "outputId": "c2913bcb-2e4d-44b9-fcde-56621d50669c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"The model has {:,} trainable parameters\".format(count_parameters(model)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 8,023,191 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW5i8cQToNWS",
        "colab_type": "code",
        "outputId": "17487196-63d3-4e52-9b1e-460de6283126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(613, 256)\n",
            "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(663, 256)\n",
            "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
            "    (fc): Linear(in_features=512, out_features=663, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whz1fXtxKPl8",
        "colab_type": "text"
      },
      "source": [
        "### Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL0kvgNaKSkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in iterator:\n",
        "\n",
        "        src = batch.src\n",
        "        trg = batch.src\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_scores_tensor = model(src, trg)\n",
        "\n",
        "        # trg: (max trg length, batch size)\n",
        "        # pred_scores_tensor: (max trg length, batch size, prob. distrib. size)\n",
        "\n",
        "        output_dim = model.decoder.output_dim\n",
        "\n",
        "        pred_scores_matrix = pred_scores_tensor[1:].view(-1, output_dim)\n",
        "\n",
        "        # <sos> is the first input and doesn't have ground truth (it's not predicted)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        # trg: ( [max trg length - 1] * batch size)\n",
        "        # pred_scores_matrix: ( [max trg length - 1] * batch size, prob.dist. size)\n",
        "\n",
        "        loss = criterion(pred_scores_matrix, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}