{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04a_NLP_and_sequence-to-sequence_RNNs.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN9cIzWuDC+kid7PvsTWE2e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/introNLP/blob/master/04a_NLP_and_sequence_to_sequence_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12TvrvBulJba",
        "colab_type": "text"
      },
      "source": [
        "# NLP and Sequence-to-Sequence RNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7C_h_GLDTbK",
        "colab_type": "code",
        "outputId": "cc4873bf-0fa8-49b9-df17-d83697ed80a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "!wget https://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-20 00:26:47--  https://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:3033::6818:6dc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4767708 (4.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   4.55M  18.8MB/s    in 0.2s    \n",
            "\n",
            "2020-02-20 00:26:52 (18.8 MB/s) - ‘spa-eng.zip’ saved [4767708/4767708]\n",
            "\n",
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejo5g5kBECxP",
        "colab_type": "code",
        "outputId": "15c489ec-e9ec-4d98-a1bf-6285d02f28c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset\n",
        "from torchtext import data\n",
        "\n",
        "import spacy\n",
        "\n",
        "from io import open\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "\n",
        "print(\"Spacy version:\", spacy.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spacy version: 2.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8wUoQ65g2u6",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the Natural Language Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAOScg5lgthG",
        "colab_type": "text"
      },
      "source": [
        "### Text Normalization functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64vE5FSE2tgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        char for char in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(char) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPudOEURc7Zl",
        "colab_type": "text"
      },
      "source": [
        "### Read Pairs function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_kg0w9K5XHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readPairs(pathToFile, slang, tlang):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    f = open(pathToFile, encoding='utf-8')\n",
        "    lines = f.read().strip().split('\\n')\n",
        "\n",
        "    pairs = [[normalizeString(s) for s in line.split('\\t')[:2]] for line in lines]\n",
        "\n",
        "    return pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-uO-hSZc2ao",
        "colab_type": "text"
      },
      "source": [
        "### Filter function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV0nAsYq8ClC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am\", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \",\n",
        "    \"would\", \"what\",\n",
        "    \"when\", \"how\"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[0].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYAokqACdAxo",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Data function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRDq95CW_ll8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(pathToFile, slang, tlang):\n",
        "    pairs = readPairs(pathToFile, slang, tlang)\n",
        "    print(\"Read {} sentence pairs\".format(len(pairs)))\n",
        "    #pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {} sentence pairs\".format(len(pairs)))\n",
        "    return pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX7QHkDddJiq",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Indexes (Splits) function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFs44kLn_INN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def datasetIndexes(pairs, train=0.8, val=0.1):\n",
        "    num_examples = len(pairs)\n",
        "    indexes = list(range(num_examples))\n",
        "    last_train_idx = round(train*num_examples)\n",
        "    last_valid_idx = last_train_idx + round(val*num_examples)\n",
        "    random.shuffle(indexes)\n",
        "    train_idxs = indexes[:last_train_idx]\n",
        "    val_idxs = indexes[last_train_idx:last_valid_idx]\n",
        "    test_idxs = indexes[last_valid_idx:]\n",
        "    return train_idxs, val_idxs, test_idxs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTKk0Hc_dedI",
        "colab_type": "text"
      },
      "source": [
        "### Spanish-English Translation Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H19pHY6QPyF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpanishEnglishDataset(TranslationDataset):\n",
        "    \"\"\"English to Spanish Dataset\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, exts, fields, root='.data/',\n",
        "               train='train', validation='val', test='test', **kwargs):\n",
        "        \n",
        "        if 'path' not in kwargs:\n",
        "            expected_folder = os.path.join(root, cls.name)\n",
        "            path = expected_folder if os.path.exists(expected_folder) else None\n",
        "        else:\n",
        "            path = kwargs['path']\n",
        "            del kwargs['path']\n",
        "        \n",
        "        return super(SpanishEnglishDataset, cls).splits(\n",
        "            exts, fields, path, root, train, validation, test, **kwargs\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QisIUx_5drmV",
        "colab_type": "text"
      },
      "source": [
        "## Spanish-English Dataset files (splits)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKp8BQP6DX6c",
        "colab_type": "code",
        "outputId": "4785b8e2-9bbf-442b-df3b-9f3207faaa53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "pairs = prepareData('spa.txt', 'eng', 'spa')\n",
        "train_idxs, val_idxs, test_idxs = datasetIndexes(pairs)\n",
        "print(\"{} {} {}\".format(len(train_idxs), len(val_idxs), len(test_idxs) ))\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 123335 sentence pairs\n",
            "Trimmed to 123335 sentence pairs\n",
            "98668 12334 12333\n",
            "['the management finally succumbed to the demand of the workers and gave them a raise .', 'la gestion finalmente sucumbio a la demanda de los trabajadores y les dio un aumento .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf-RZx3YGSDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('train.en', 'w') as slang_file, open('train.es', 'w') as tlang_file:\n",
        "    for i in train_idxs:\n",
        "        slang_file.write(pairs[i][0] + '\\n')\n",
        "        tlang_file.write(pairs[i][1] + '\\n')\n",
        "\n",
        "with open('val.en', 'w') as slang_file, open('val.es', 'w') as tlang_file:\n",
        "    for i in val_idxs:\n",
        "        slang_file.write(pairs[i][0] + '\\n')\n",
        "        tlang_file.write(pairs[i][1] + '\\n')\n",
        "\n",
        "with open('test.en', 'w') as slang_file, open('test.es', 'w') as tlang_file:\n",
        "    for i in test_idxs:\n",
        "        slang_file.write(pairs[i][0] + '\\n')\n",
        "        tlang_file.write(pairs[i][1] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SPfenbVeFlS",
        "colab_type": "text"
      },
      "source": [
        "## Loading the files (train, val, test) and Creating a SpanishDatatset instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jBTDFm1jgS8A",
        "outputId": "bbfa0492-c32f-4e54-daa0-a089813d4684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download es"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: es_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.1.0/es_core_news_sm-2.1.0.tar.gz#egg=es_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/es_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/es\n",
            "You can now load the model via spacy.load('es')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y03SbEQ1X54e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_es = spacy.load('es')\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenize_es(text):\n",
        "    \"\"\"\n",
        "    Tokenizes Spanish text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [token.text for token in spacy_es.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [token.text for token in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-37Cb1XYY-08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = data.Field(tokenize=tokenize_en, init_token='<SOS>', eos_token='<EOS>', lower=True)\n",
        "TRG = data.Field(tokenize=tokenize_es, init_token='<SOS>', eos_token='<EOS>', lower=True)\n",
        "\n",
        "train_data, valid_data, test_data = SpanishEnglishDataset.splits(\n",
        "    path='',\n",
        "    exts=('.en', '.es'),\n",
        "    fields=(SRC, TRG)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYPDAg7YiSu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq=5)\n",
        "TRG.build_vocab(train_data, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVGpH1PiaLpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QIyVoSKcIfH",
        "colab_type": "code",
        "outputId": "c176d0a7-8ee9-4878-d2de-72ccc50d0794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(len(test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98668\n",
            "12334\n",
            "12333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRCx6WAEeg5P",
        "colab_type": "text"
      },
      "source": [
        "## Vocabulary and Training Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlMdtj01WXIU",
        "colab_type": "code",
        "outputId": "49ae7ea9-15ad-424a-f93b-ee407a9eb8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "SRC.vocab.itos[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', '<SOS>', '<EOS>', '.', 'i', 'the', 'to', 'you', 'tom']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7DsRjXshe0B",
        "colab_type": "code",
        "outputId": "d2600fe4-608e-4b24-8a34-3abc6511abd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "batch = next(iter(train_iterator))\n",
        "\n",
        "src_example = batch.src[:, 0]\n",
        "src = ' '.join(map(lambda i: SRC.vocab.itos[i], src_example))\n",
        "trg_example = batch.trg[:, 0]\n",
        "trg = ' '.join(map(lambda i: TRG.vocab.itos[i], trg_example))\n",
        "\n",
        "print(\"English sentence:\", src)\n",
        "print(\"Spanish sentence:\", trg)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English sentence: <SOS> you have until midnight . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Spanish sentence: <SOS> tenes hasta medianoche . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp2GwmBkcOsm",
        "colab_type": "code",
        "outputId": "0af98b8d-6960-4911-e470-ac828a17cbb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(SRC.vocab.freqs.most_common(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.', 85805), ('i', 28905), ('the', 23337), ('to', 22119), ('you', 20269), ('tom', 16949), ('a', 14944), ('?', 13157), ('t', 12197), ('is', 12022)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJMj5_jRcdte",
        "colab_type": "code",
        "outputId": "de3948c2-b818-440d-8e5e-62743188f0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(TRG.vocab.freqs.most_common(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.', 85461), ('que', 21836), ('de', 18965), ('el', 18433), ('no', 17094), ('a', 16918), ('tom', 16127), ('la', 14933), ('?', 13174), ('es', 10342)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wz-vwqTiVZS",
        "colab_type": "text"
      },
      "source": [
        "## Sequence-to-Sequence (Seq2Seq) Model (without Attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIw5655Ln_pT",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdvU822UiiY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim,\n",
        "                 hidden_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                           num_layers, dropout=dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src_sequences):\n",
        "\n",
        "        # src_sequences: (max sequences length, batch size)\n",
        "\n",
        "        seq_embeddings = self.dropout(self.embedding(src_sequences))\n",
        "\n",
        "        # src_seq_embeddings: (max sequences length, batch size, embedding dimension)\n",
        "\n",
        "        seq_hidden, (hidden, cell) = self.rnn(seq_embeddings)\n",
        "\n",
        "        # hidden = (num layers * num directions, batch size, hidden dim)\n",
        "        # cell = (num layers * num directions, batch size, hidden dim)\n",
        "\n",
        "        return hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xCwi9fDn6g2",
        "colab_type": "text"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The Decoder is also built using an RNN and it represents a Language Model. As we saw in the first notebook, language modeling is the task of predicting the next word given a sequence of previous words:\n",
        "\n",
        "\n",
        "$$ \\Large P(y^{(t+1)}|y^{(1)}, ...,y^{(t)}) $$\n",
        "\n",
        "In this case, our Decoder represents a Conditional Language Model\n",
        "\n",
        "$$ \\Large P(y|x) = P(y_1|x)P(y_2|y_1,x)P(y_3|y_1,y_2,x)...P(y_T|y_1,...,y_{T-1},x) $$\n",
        "\n",
        "And we want to find a translation $y$ that maximizes\n",
        "\n",
        "$$ \\Large P(y|x) =  \\displaystyle \\prod_{t=1}^{T} P(y_t|y_1,...,y_{t-1}, x) $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCJ8RGlrTql-",
        "colab_type": "text"
      },
      "source": [
        "#### Negative log likelihood\n",
        "\n",
        "When training our model, in each step $t$ it tries to minimize the negative log likelihood loss of the next word to be the ground truth:\n",
        "\n",
        "$$ \\large L_{CE} (\\hat{y}^{(t)},y^{(t)}) = -\\log \\hat{y}_{s_{t+1}}^{(t)}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mnj77LceFBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embedding_dim,\n",
        "                 hidden_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_di = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                           num_layers, dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, seq_at_t, hidden, cell):\n",
        "\n",
        "        # seq_at_t: (batch size) -> vector of sequences at time t\n",
        "\n",
        "        seq_at_t = seq_at_t.unsqueeze(0)\n",
        "\n",
        "        # seq_at_t: (1, batch size)\n",
        "\n",
        "        seq_at_t_embedded = self.dropout(self.embedding(seq_at_t))\n",
        "\n",
        "        # seq_at_t_embedded: (1, batch size, embedding dim)\n",
        "        # hidden = (num layers * num directions, batch size, hidden dim)\n",
        "        # cell = (num layers * num directions, batch size, hidden dim)\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(seq_at_t_embedded, (hidden, cell))\n",
        "\n",
        "        # num directions = 1\n",
        "        # output: (1, batch size, hidden dim)\n",
        "\n",
        "        pred_scores = self.fc(output.squeeze(0))\n",
        "\n",
        "        # pred_scores: (batch size, output dim)\n",
        "        # hidden = (num layers , batch size, hidden dim)\n",
        "        # cell = (num layers , batch size, hidden dim)\n",
        "\n",
        "        return pred_scores, hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUF3H8jGv-XF",
        "colab_type": "text"
      },
      "source": [
        "### Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJDsq0ruwEtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "\n",
        "        # src: (max src length, batch size)\n",
        "        # trg: (max trg length, batch size)\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # outputs: tensor to store the decoder output\n",
        "        # outputs: (target max length, batch size, target vocab size)\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # we'll plug the last hidden states of the encoder in the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # the first target sequence token: <sos>\n",
        "        seq_at_t = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            pred_scores, hidden, cell = self.decoder(seq_at_t, hidden, cell)\n",
        "\n",
        "            outputs[t] = pred_scores\n",
        "\n",
        "            # teacher_force: Boolean\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # argmax_token: Token\n",
        "            argmax_token = pred_scores.argmax(1)\n",
        "\n",
        "            # depending on teacher_forcing value, use the next token or argmax_token\n",
        "            seq_at_t = trg[t] if teacher_force else argmax_token\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glalL9ooiv0Y",
        "colab_type": "text"
      },
      "source": [
        "### Model: 2 LSTM layers, no pre-trained Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWjdBpS2jRkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab) # output_dim is both the output dim and target vocab size\n",
        "ENCODER_EMB_DIM = 256 # length of the encoder's embedding vectors\n",
        "DECODER_EMB_DIM = 256 # length of the decoder's embedding vectors\n",
        "HIDDEN_DIM = 512 # RNN's hidden units\n",
        "NUM_LAYERS = 2 # number of stacked LSTM layers\n",
        "ENCODER_DROPOUT = 0.5 # encoder's dropout probability\n",
        "DECODER_DROPOUT = 0.5 # decoder's dropout probability\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, ENCODER_EMB_DIM, HIDDEN_DIM, NUM_LAYERS, ENCODER_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DECODER_EMB_DIM, HIDDEN_DIM, NUM_LAYERS, DECODER_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVO5JJfAmq4x",
        "colab_type": "code",
        "outputId": "fb559f50-5e26-439c-d2e7-5f7774d89886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"The model has {:,} trainable parameters\".format(count_parameters(model)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 13,780,836 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-c5JHXUA2Pk",
        "colab_type": "code",
        "outputId": "4484c210-16a1-4aea-b85b-8370dc90c8a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "def init_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(4801, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(6756, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=512, out_features=6756, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW5i8cQToNWS",
        "colab_type": "code",
        "outputId": "57f412cf-e0de-4273-f82a-1c015367aab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(4801, 256)\n",
            "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(6756, 256)\n",
            "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
            "    (fc): Linear(in_features=512, out_features=6756, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A__RFghi8jKE",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIyvW6ua8osX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whz1fXtxKPl8",
        "colab_type": "text"
      },
      "source": [
        "### Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL0kvgNaKSkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in iterator:\n",
        "\n",
        "        src = batch.src\n",
        "        trg = batch.src\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_scores_tensor = model(src, trg)\n",
        "\n",
        "        # trg: (max trg length, batch size)\n",
        "        # pred_scores_tensor: (max trg length, batch size, prob. distrib. size)\n",
        "\n",
        "        output_dim = model.decoder.output_dim\n",
        "\n",
        "        pred_scores_matrix = pred_scores_tensor[1:].view(-1, output_dim)\n",
        "\n",
        "        # <sos> is the first input and doesn't have ground truth (it's not predicted)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        # trg: ( [max trg length - 1] * batch size)\n",
        "        # pred_scores_matrix: ( [max trg length - 1] * batch size, prob.dist. size)\n",
        "\n",
        "        loss = criterion(pred_scores_matrix, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXIWpuy_0Ifs",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbe4BQ_M0Mro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            pred_scores_tensor = model(src, trg, 0)\n",
        "\n",
        "            # trg: (max trg length, batch size)\n",
        "            # pred_scores_tensor: (max trg length, batch size, prob. distrib. size)\n",
        "\n",
        "            output_dim = model.decoder.output_dim\n",
        "\n",
        "            pred_scores_matrix = pred_scores_tensor[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "            \n",
        "            # trg: ( [max trg length - 1] * batch size)\n",
        "            # pred_scores_matrix: ( [max trg length - 1] * batch size, prob.dist. size)\n",
        "\n",
        "            loss = criterion(pred_scores_matrix, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoVSP35T36Tr",
        "colab_type": "text"
      },
      "source": [
        "### Epoch Timer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-SrYzSF4AJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins, elapsed_secs = divmod(int(elapsed_time), 60)\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrNKKyWX4IAK",
        "colab_type": "text"
      },
      "source": [
        "### Training-Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf-iALdx4L35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_evaluate(model, optimizer, criterion, n_epochs, clip):\n",
        "\n",
        "    metrics = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_pplx\": [],\n",
        "        \"valid_loss\": [],\n",
        "        \"valid_pplx\": [],\n",
        "    }\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss = train(model, train_iterator, optimizer, criterion, clip)\n",
        "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "        train_pplx = math.exp(train_loss)\n",
        "        valid_pplx = math.exp(valid_loss)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        print(\"Epoch: {:02} | Epoch Time: {}m {}s\".format(epoch+1, epoch_mins, epoch_secs))\n",
        "        print(\"\\tTrain Loss: {:.3f} | Train Acc: {:7.3f}\".format(train_loss, train_pplx))\n",
        "        print(\"\\t Val. Loss: {:.3f} |  Val. Acc: {:7.3f}\".format(valid_loss, valid_pplx))\n",
        "\n",
        "        metrics[\"train_loss\"].append(train_loss)\n",
        "        metrics[\"train_pplx\"].append(train_pplx)\n",
        "        metrics[\"valid_loss\"].append(valid_loss)\n",
        "        metrics[\"valid_pplx\"].append(valid_pplx)\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhUOhde57ULg",
        "colab_type": "text"
      },
      "source": [
        "### Plot Metrics function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IbjVjKT7Yr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics(metrics):\n",
        "\n",
        "    train_loss = metrics['train_loss']\n",
        "    valid_loss = metrics['valid_loss']\n",
        "    train_pplx = metrics['train_pplx']\n",
        "    valid_pplx = metrics['valid_pplx']\n",
        "\n",
        "    epochs = range(1, N_EPOCHS + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, valid_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_pplx, 'b', label='Training perplexity')\n",
        "    plt.plot(epochs, valid_pplx, 'r', label='Validation perplexity')\n",
        "    plt.title('Training and validation perplexity')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Perplexity')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx9InwwD-FPR",
        "colab_type": "text"
      },
      "source": [
        "### Training a Seq2Seq Model from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "590ENoA7-KeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = train_evaluate(model, optimizer, criterion, N_EPOCHS, CLIP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPGpG8q0-m7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "test_pplx = math.exp(test_loss)\n",
        "\n",
        "print(\"Train Loss: {:.3f} | Train Acc: {:7.3f}%\".format(test_loss, test_pplx))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}