{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04a_NLP_and_sequence-to-sequence_RNNs.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMp8czRzH6NPagc3/+BwC88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/introNLP/blob/master/04a_NLP_and_sequence_to_sequence_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12TvrvBulJba",
        "colab_type": "text"
      },
      "source": [
        "# NLP and Sequence-to-Sequence RNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7C_h_GLDTbK",
        "colab_type": "code",
        "outputId": "1a5d06e9-80f9-4cc1-c49c-a36f573b8cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "!wget https://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-15 01:50:36--  https://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:3037::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4767708 (4.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip           9%[>                   ] 459.61K  2.24MB/s               \rspa-eng.zip         100%[===================>]   4.55M  13.7MB/s    in 0.3s    \n",
            "\n",
            "2020-02-15 01:50:37 (13.7 MB/s) - ‘spa-eng.zip’ saved [4767708/4767708]\n",
            "\n",
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-va0tQ1eYb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "e6450088-06dc-4198-866e-bfa6c067e266"
      },
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Collecting es_core_news_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.1.0/es_core_news_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 1.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.1.0-cp36-none-any.whl size=11111556 sha256=98a2e012517fbdd6670e93902ae48a32496c35fb25d6bd55ce5b09c0a0abcefb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qzauv6bj/wheels/cc/ee/c4/68922955901918a9aaa82e828d4f7ee1ccfc861285277e79b7\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejo5g5kBECxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4944500-b225-4a14-81c2-349f9c657552"
      },
      "source": [
        "import torch\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset\n",
        "from torchtext import data\n",
        "\n",
        "import spacy\n",
        "\n",
        "from io import open\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "print(\"Spacy version:\", spacy.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spacy version: 2.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64vE5FSE2tgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        char for char in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(char) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_kg0w9K5XHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readPairs(pathToFile, slang, tlang):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    f = open(pathToFile, encoding='utf-8')\n",
        "    lines = f.read().strip().split('\\n')\n",
        "\n",
        "    pairs = [[normalizeString(s) for s in line.split('\\t')[:2]] for line in lines]\n",
        "\n",
        "    return pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV0nAsYq8ClC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am\", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[0].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRDq95CW_ll8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(pathToFile, slang, tlang):\n",
        "    pairs = readPairs(pathToFile, slang, tlang)\n",
        "    print(\"Read {} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {} sentence pairs\".format(len(pairs)))\n",
        "    return pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFs44kLn_INN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def datasetIndexes(pairs, train=0.8, val=0.1):\n",
        "    num_examples = len(pairs)\n",
        "    indexes = list(range(num_examples))\n",
        "    last_train_idx = round(train*num_examples)\n",
        "    last_valid_idx = last_train_idx + round(val*num_examples)\n",
        "    random.shuffle(indexes)\n",
        "    train_idxs = indexes[:last_train_idx]\n",
        "    val_idxs = indexes[last_train_idx:last_valid_idx]\n",
        "    test_idxs = indexes[last_valid_idx:]\n",
        "    return train_idxs, val_idxs, test_idxs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H19pHY6QPyF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpanishDataset(TranslationDataset):\n",
        "    \"\"\"English to Spanish Dataset\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, exts, fields, root='.data/',\n",
        "               train='train', validation='val', test='test', **kwargs):\n",
        "        \n",
        "        if 'path' not in kwargs:\n",
        "            expected_folder = os.path.join(root, cls.name)\n",
        "            path = expected_folder if os.path.exists(expected_folder) else None\n",
        "        else:\n",
        "            path = kwargs['path']\n",
        "            del kwargs['path']\n",
        "        \n",
        "        return super(SpanishDataset, cls).splits(\n",
        "            exts, fields, path, root, train, validation, test, **kwargs\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKp8BQP6DX6c",
        "colab_type": "code",
        "outputId": "d2ff2bca-efd1-4a2e-be79-086c793077f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "pairs = prepareData('spa.txt', 'eng', 'spa')\n",
        "train_idxs, val_idxs, test_idxs = datasetIndexes(pairs)\n",
        "print(\"{} {} {}\".format(len(train_idxs), len(val_idxs), len(test_idxs) ))\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 123335 sentence pairs\n",
            "Trimmed to 7588 sentence pairs\n",
            "6070 759 759\n",
            "['i m sure everything will be fine .', 'estoy segura de que todo ira bien .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf-RZx3YGSDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('train.en', 'w') as slang_file, open('train.es', 'w') as tlang_file:\n",
        "    for i in train_idxs:\n",
        "        slang_file.write(pairs[i][0] + '\\n')\n",
        "        tlang_file.write(pairs[i][1] + '\\n')\n",
        "\n",
        "with open('val.en', 'w') as slang_file, open('val.es', 'w') as tlang_file:\n",
        "    for i in val_idxs:\n",
        "        slang_file.write(pairs[i][0] + '\\n')\n",
        "        tlang_file.write(pairs[i][1] + '\\n')\n",
        "\n",
        "with open('test.en', 'w') as slang_file, open('test.es', 'w') as tlang_file:\n",
        "    for i in test_idxs:\n",
        "        slang_file.write(pairs[i][0] + '\\n')\n",
        "        tlang_file.write(pairs[i][1] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y03SbEQ1X54e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import es_core_news_sm as language_es\n",
        "\n",
        "spacy_es = language_es.load()\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_es(text):\n",
        "    \"\"\"\n",
        "    Tokenizes Spanish text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [token.text for token in spacy_es.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [token.text for token in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G1OJirugl6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "4db2bb66-b812-4f21-82e9-a9a4256fb51b"
      },
      "source": [
        "!pwd\n",
        "!ls -l"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "total 23064\n",
            "-rw-r--r-- 1 root root     1441 Jan 11 23:49 _about.txt\n",
            "drwxr-xr-x 1 root root     4096 Feb  5 18:37 sample_data\n",
            "-rw-r--r-- 1 root root  4767708 Jan 11 14:49 spa-eng.zip\n",
            "-rw-r--r-- 1 root root 18432706 Jan 11 23:49 spa.txt\n",
            "-rw-r--r-- 1 root root    19743 Feb 15 01:50 test.en\n",
            "-rw-r--r-- 1 root root    20827 Feb 15 01:50 test.es\n",
            "-rw-r--r-- 1 root root   154494 Feb 15 01:50 train.en\n",
            "-rw-r--r-- 1 root root   163606 Feb 15 01:50 train.es\n",
            "-rw-r--r-- 1 root root    19472 Feb 15 01:50 val.en\n",
            "-rw-r--r-- 1 root root    20478 Feb 15 01:50 val.es\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-37Cb1XYY-08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = data.Field(tokenize=tokenize_en, init_token='<SOS>', eos_token='<EOS>', lower=True)\n",
        "TRG = data.Field(tokenize=tokenize_es, init_token='<SOS>', eos_token='<EOS>', lower=True)\n",
        "\n",
        "train_data, valid_data, test_data = SpanishDataset.splits(\n",
        "    path='',\n",
        "    exts=('.en', '.es'),\n",
        "    fields=(SRC, TRG)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYPDAg7YiSu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq=5)\n",
        "TRG.build_vocab(train_data, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVGpH1PiaLpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QIyVoSKcIfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c55e5694-b566-4089-aa1d-0fc1a3f64145"
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(valid_data))\n",
        "print(len(test_data))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6070\n",
            "759\n",
            "759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7DsRjXshe0B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "66f04357-dab6-4dd1-fa8e-0ec544112a31"
      },
      "source": [
        "batch = next(iter(train_iterator))\n",
        "\n",
        "print(batch.src[:,0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  2,  21,   7, 295,   4,   3,   1,   1,   1,   1,   1],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}