{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03b_NLP_and_recurrent_neural_networks.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPnjGo8PQ8d3pvTq0zLT6lh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/introNLP/blob/master/03b_NLP_and_recurrent_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdoE59GqVfhi",
        "colab_type": "text"
      },
      "source": [
        "# Pre-trained Word Embeddings and RNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dRBQ88DscTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "SEED = 3773\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS7TAPJruzr7",
        "colab_type": "code",
        "outputId": "5d3e3537-beeb-4863-9ad1-be2d207ee57a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 11.6MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDDzCMY3u-46",
        "colab_type": "code",
        "outputId": "40765e40-2953-4262-8267-894a82dd2400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Number of training examples: {}\".format(len(train_data)))\n",
        "print(\"Number of testing examples: {}\".format(len(test_data)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ572Bd_wC6h",
        "colab_type": "code",
        "outputId": "b7548441-f242-47fa-fa93-064c3bdded77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['Diagnosis', 'Murder', 'has', 'been', 'shown', 'on', 'most', 'Weekday', 'afternoons', 'on', 'BBC1', 'since', 'I', 'used', 'to', 'watch', 'it', 'while', 'ill', 'from', 'School', 'a', 'good', '10', 'years', 'ago', '-', 'I', 'know', 'I', 'should', \"n't\", 'really', 'enjoy', 'it', ',', 'in', 'the', 'same', 'way', 'I', 'should', \"n't\", 'enjoy', \"'\", 'Murder', 'she', 'Wrote', \"'\", 'but', 'I', \"'m\", 'totally', 'addicted', 'to', 'both', 'and', 'even', 'have', 'the', 'DVD', 'box', '-', 'sets', '....', 'OK', 'I', 'know', 'that', \"'s\", 'sad!<br', '/><br', '/>Dick', 'Van', 'Dyke', 'carries', 'the', 'show', 'as', 'he', 'stars', 'as', 'Dr', '.', 'Mark', 'Sloan', 'a', 'Doctor', 'at', 'Community', 'General', 'Hospital', 'in', 'L.A', 'who', 'is', 'also', 'a', 'Police', 'consultant', 'for', 'the', 'L.A.P.D.', '-', 'his', 'son', 'Steve', '(', 'Barry', 'van', 'Dyke', '-', 'Dick', \"'s\", 'real', 'life', 'son', ')', 'is', 'a', 'Police', 'Officer', ',', 'who', 'needs', 'his', 'father', \"'s\", 'help', 'on', 'very', 'many', 'Suspicious', 'deaths', '.', '<', 'br', '/><br', '/>Along', 'for', 'the', 'ride', 'is', 'Dr', '.', 'Amanda', 'Bentley', '(', 'Victoria', 'Bentley', ')', 'the', 'resident', 'Pathologist', 'at', 'Community', 'General', 'and', 'for', 'the', 'first', 'couple', 'of', 'seasons', 'you', 'had', 'Scott', 'Baio', 'playing', 'Dr', '.', 'Jack', 'Stewart', ',', 'who', 'upped', 'and', 'left', 'the', 'series', 'in', '1995', 'hoping', 'to', 'go', 'on', 'to', 'bigger', 'and', 'better', 'things', '...', 'he', 'should', 'have', 'stayed', 'where', 'he', 'was', ',', 'he', 'has', \"n't\", 'done', 'anything', 'of', 'note', 'since', '....', 'and', 'his', 'only', 'theatrical', 'appearance', 'for', 'many', 'years', 'was', 'in', 'Baby', 'Geniuses', '2:Superbabies', '....', 'Oh', 'Dear!!!<br', '/><br', '/>anyhow', 'Dr', '.', 'Jack', 'Stewart', 'was', 'replaced', 'by', 'the', 'younger', 'Dr', '.', 'Jesse', 'Travis', 'played', 'by', 'Charlie', 'Schlatter', 'who', 'stepped', 'into', 'Baio', \"'s\", 'shoes', 'pretty', 'comfortably.<br', '/><br', '/>The', 'series', 'is', 'highly', 'implausible', 'but', 'what', 'Whodunit', 'series', 'is', \"n't\", '?', '(', 'Murder', 'she', 'wrote', '-', 'everywhere', 'Jessica', 'goes', ',', 'someone', 'ends', 'up', 'dead', ',', 'or', 'The', 'underrated', 'Father', 'Dowling', 'Mysteries', 'about', 'a', 'Murder', 'solving', 'Priest', 'with', 'nun', 'sidekick)<br', '/><br', '/>The', 'series', 'was', 'much', 'lighter', 'up', 'until', '1997', 'this', 'is', 'because', 'it', 'had', 'a', 'supporting', 'cast', 'that', 'included', 'the', 'bumbling', 'Hospital', 'Manager', 'Norman', 'Briggs', 'played', 'by', 'Michael', 'Tucci', 'along', 'with', 'Nurse', '&', 'Mark', \"'s\", 'secretary', 'Dolores', 'played', 'by', 'Delores', 'Hall', ',', 'After', '1997', 'both', 'these', 'characters', 'were', 'no', 'longer', 'included', 'and', 'the', 'series', 'became', 'a', 'grittier', 'affair', 'with', 'a', 'bigger', 'looking', 'budget', ',', 'some', 'episodes', 'included', 'far', 'more', 'action', ',', 'one', 'episode', 'the', 'entire', 'Hospital', 'is', 'blown', 'up.<br', '/><br', '/>This', 'was', 'a', 'family', 'show', 'For', 'the', 'Van', 'Dyke', \"'s\", 'because', 'as', 'well', 'as', 'Dick', \"'s\", 'Son', 'Barry', ',', 'you', 'also', 'had', 'Dick', \"'s\", 'Daughter', 'And', 'all', 'his', 'Grandchildren', 'making', 'an', 'appearance', 'in', 'various', 'episodes', '.', '<', 'br', '/><br', '/>As', 'the', 'series', 'went', 'on', 'it', 'got', 'a', 'bit', 'silly', ',', 'one', 'episode', 'I', 'remember', 'Dick', 'van', 'Dike', 'plays', 'his', 'entire', 'family', ',', 'which', 'was', 'a', 'bit', 'out', 'of', 'the', 'ordinary', ',', 'but', 'on', 'the', 'Whole', \"'\", 'Diagnosis', 'Murder', \"'\", 'was', 'a', 'really', 'good', 'TV', 'show', 'which', 'had', 'numerous', 'good', 'Guest', 'Stars.<br', '/><br', '/>Since', 'this', 'show', 'finished', 'in', '2001', ',', 'Dick', '&', 'Barry', 'have', 'appeared', 'together', 'again', 'in', 'the', \"'\", 'MURDER', '101', \"'\", 'series', 'of', 'TV', 'Movies', 'made', 'by', 'The', 'Hallmark', 'Channel', ',', 'pretty', 'much', 'following', 'the', 'same', 'path', ',', 'and', 'still', 'enjoyable', '.', 'Dick', 'who', \"'s\", 'now', 'in', 'his', 'mid', '80', \"'s\", 'does', \"n't\", 'seem', 'to', 'change', 'a', 'great', 'deal', ',', 'and', 'looks', 'as', 'if', 'he', \"'ll\", 'be', 'working', 'till', 'the', 'bitter', 'end.<br', '/><br', '/>TV', 'SHOW', '*', '*', '*', '*', 'OUT', 'OF', '*', '*', '*', '*', '*'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM5CtAGe2eho",
        "colab_type": "code",
        "outputId": "51f7e950-b09e-4bbc-9136-08877dfed3a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import random\n",
        "\n",
        "train_data, valid_data = train_data.split(\n",
        "    random_state=random.seed(SEED),\n",
        "    split_ratio=0.8\n",
        ")\n",
        "\n",
        "print(\"Number of training examples: {}\".format(len(train_data)))\n",
        "print(\"Number of validation examples: {}\".format(len(valid_data)))\n",
        "print(\"Number of testing examples: {}\".format(len(test_data)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 20000\n",
            "Number of validation examples: 5000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nul-rT84zAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 8185\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXyHKPd054Am",
        "colab_type": "code",
        "outputId": "1ebfc027-2abb-480b-c542-7af96acefc4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Unique tokens in TEXT vocabulary: {}\".format(len(TEXT.vocab)))\n",
        "print(\"Unique tokens in LABEL vocabulary: {}\".format(len(LABEL.vocab)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 8187\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAb8f32B6Jcp",
        "colab_type": "code",
        "outputId": "e019c2b3-39c4-4815-c852-1a3d03288ca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 232678), (',', 220840), ('.', 188920), ('and', 125362), ('a', 125266), ('of', 115884), ('to', 107654), ('is', 87196), ('in', 70206), ('I', 62349), ('it', 61298), ('that', 56438), ('\"', 50419), (\"'s\", 49667), ('this', 48419), ('-', 41945), ('/><br', 41022), ('was', 40196), ('as', 35006), ('with', 34063)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA--x2i66ejW",
        "colab_type": "code",
        "outputId": "08cfc8f0-9d6d-465e-a9e8-97adb1e4d560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cCa8T_t8Tf9",
        "colab_type": "code",
        "outputId": "add46b11-ed27-4bcb-a846-0d7aabce9cc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f51c731e378>, {'pos': 0, 'neg': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVtV1PKW9nLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIg0PitTn1or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim,\n",
        "                 hidden_dim, output_dim,\n",
        "                 n_layers, bidirectional):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "\n",
        "        # sequences: (max sequences length, batch size)\n",
        "        seq_embeddings = self.embedding(sequences)\n",
        "\n",
        "        # seq_embeddings: (max sequences length, batch size, embedding dim)\n",
        "        seq_hidden, (hidden, cell) = self.rnn(seq_embeddings)\n",
        "\n",
        "        # hidden: (num_layers * num_directions, batch size, hidden dim)\n",
        "        hidden_concat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "\n",
        "        # hidden_concat: (batch size, num_directions * hidden dim)\n",
        "        output = self.fc(hidden_concat)\n",
        "\n",
        "        # output: (batch size, 1) -> vector\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHQT54qZNjco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 1\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "model = RNN(INPUT_DIM,\n",
        "            EMBEDDING_DIM,\n",
        "            HIDDEN_DIM,\n",
        "            OUTPUT_DIM,\n",
        "            N_LAYERS,\n",
        "            BIDIRECTIONAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Noi_YI-O2a",
        "colab_type": "code",
        "outputId": "68d458c0-15ce-40ee-d745-8124090db377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "iterator = iter(train_iterator)\n",
        "batch = next(iterator)\n",
        "print(batch.text.shape)\n",
        "print(batch.text[:,0].shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([592, 64])\n",
            "torch.Size([592])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltpol6D2O7ol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbfb0c5f-998c-42fb-ef98-6bddc3484bfa"
      },
      "source": [
        "model = model.to(device)\n",
        "\n",
        "output = model(batch.text)\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}