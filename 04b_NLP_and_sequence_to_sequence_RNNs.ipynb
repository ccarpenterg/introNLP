{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04b_NLP_and_sequence_to_sequence_RNNs.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPWHpLW3ED0P1Rek27pFyro",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/introNLP/blob/master/04b_NLP_and_sequence_to_sequence_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjG5aYEIjAHN",
        "colab_type": "text"
      },
      "source": [
        "# Neural Machine Translation with Seq2Seq and Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtpR25QcmjyW",
        "colab_type": "text"
      },
      "source": [
        "Traditionally neural machine translation uses the encoder-decoder architecture that encodes the source sentence into a vector which then is fed to a decoder which generates the translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7iQJuNYqDWT",
        "colab_type": "text"
      },
      "source": [
        "![encoder-decoder](https://user-images.githubusercontent.com/114733/80140098-78a57a80-8575-11ea-90e9-35a961d4acf4.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAbylmompHJn",
        "colab_type": "text"
      },
      "source": [
        "But according to Bahdanau 2016, this fixed-length vector represents a bottleneck. Moreover, one of the issues with the encoder-decoder model is that it needs to compress all the necesarry information of the source sentence into this aforementioned vector, which can be difficult especially for longer sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImIvkb4XVein",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq with Attention\n",
        "\n",
        "In the paper **Neural Machine Translation by Jointly Learning to Align and Translate**, Bahdanau proposed to extend the encoder-decoder architecture by alowwing the model to automatically soft search for part of the source sentence that are relevant to predicting the target word.\n",
        "\n",
        "### Attention\n",
        "\n",
        "This mechanism of soft searching for the parts of the source sentence that are relevant to a word in the target sentence is called **Attention** and it's the cornerstone of the state-of-the-art NLP models called Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH6H_8TmeX0-",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://user-images.githubusercontent.com/114733/80148688-6b8f8800-8583-11ea-84f3-fb1d464bcebf.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylIb73Er2f7Z",
        "colab_type": "text"
      },
      "source": [
        "# Sequence to Sequence with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYpTNmlMzLPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import unicodedata\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWNBDrF1zoJQ",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW01zIIGzzQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(\n",
        "            tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values))\n",
        "        )\n",
        "\n",
        "        attention_weigths = tf.nn.softmax(scores, axis=1)\n",
        "\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHc-qv-Az7L0",
        "colab_type": "text"
      },
      "source": [
        "## Encoder-Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLrl5WpljGmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            self.enc_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform'\n",
        "        )\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_size, self.enc_units))\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            self.dec_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform'\n",
        "        )\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=1)\n",
        "        output, state = self.gru(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXRjE2yQ2acg",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] Neural Machine Translation by Jointly Learning Align and Translate (Seq2Seq with Attention paper), [Bahdanau 2016](https://arxiv.org/pdf/1409.0473.pdf)\n",
        "\n",
        "[2] Neural Machine Translation with Attention ([Tensorflow official documentation](https://www.tensorflow.org/tutorials/text/nmt_with_attention))\n",
        "\n",
        "[3] Machine Translation, Seq2Seq and Attention ([slides](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture08-nmt.pdf )) ([video](https://youtu.be/XXtpJxZBa2c)). CS224n: NLP with Deep Learning (2019), Stanford University\n"
      ]
    }
  ]
}