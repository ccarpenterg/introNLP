{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01a_intro_NLP_and_word_embeddings.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/introNLP/blob/master/01a_intro_NLP_and_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTveffOPrERD",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to NLP with Deep Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18CvZLpxUWag",
        "colab_type": "text"
      },
      "source": [
        "## Language Modeling\n",
        "\n",
        "\"Language modeling is the task of assigning a probability to sentences in a language (what is the probability of seeing the sentence *the lazy dog barked loudly*?). Besides assigning a probability to each sequence of words, the language models also assign a probability for the likelihood of a given word (or sequence of words) to follow a sequence of words (what is the probability of seeing the word *barked* after seeing the sequence *the lazy dog*?).\" [1]\n",
        "\n",
        "Formally, the probability of a sequence of words $\\Large w_{1:n}$, ussing the chain rule of probability, is:\n",
        "\n",
        "$$\\Large P(w_{1:n}) = P(w_1)P(w_2|w_1)P(w_3|w_{1:2})...P(w_n|w_{1:n-1})$$\n",
        "\n",
        "which can be written using the product of sequences symbol:\n",
        "\n",
        "$$ \\Large P(w_{1:n}) = \\displaystyle \\prod_{k=1}^{n} P(w_{k} | w_{1:k-1}) $$\n",
        "\n",
        "where $\\Large w_{1:n}$ is a sequence of words: $\\Large w_1 w_2 w_3 ... w_{n-1} w_{n}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaIEe97WuzCI",
        "colab_type": "text"
      },
      "source": [
        "### N-grams\n",
        "\n",
        "N-grams are models that approximate the probability of a word given all the previous words $ P(w_{n} | w_{1:n-1}) $ by only using the conditional probability  of the preceding  $ N - 1 $ words.\n",
        "\n",
        "**Bigram (2-grams) Model**\n",
        "\n",
        "For $N=2$ we have:\n",
        "\n",
        "$$\\Large P(w_n | w_{1:n-1}) \\approx P(w_n | w_{n-1})$$\n",
        "\n",
        "now we replace our LM's sequence with this bigram approximation:\n",
        "\n",
        "$$ \\Large P(w_{1:n}) = \\displaystyle \\prod_{k=1}^{n} P(w_{k} | w_{k-1}) $$\n",
        "\n",
        "**Trigram (3-grams) Model**\n",
        "\n",
        "For $N=3$ we have:\n",
        "\n",
        "$$\\Large P(w_n | w_{1:n-1}) \\approx P(w_n | w_{n-2:n-1})$$\n",
        "\n",
        "and we do the same using this trigram approximation:\n",
        "\n",
        "$$ \\Large P(w_{1:n}) = \\displaystyle \\prod_{k=1}^{n} P(w_{k} | w_{k-2:k-1}) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhQ9cumpPtfY",
        "colab_type": "text"
      },
      "source": [
        "**A concrete N-gram example**\n",
        "\n",
        "Now we take a look at how our model works using the following sentence:\n",
        "\n",
        "**\"The Industrial Revolution began in the 18th century, when agricultural societies became more industrialized and urban.\"**\n",
        "\n",
        "3-grams for this sequence of words:\n",
        "\n",
        "**[Revolution, (The, Industrial)], [began, (Industrial, Revolution)], [in, (Revolution, began)], [the, (began, in)], [18th, (in, the)], [century, (the, 18th)]**, etc.\n",
        "\n",
        "So let's say we are building  a text editor that is able to suggest the next word given some preceding words, and in order to achieve this we need to calculate the following probability:\n",
        "\n",
        "$$\\normalsize P(\\text{century}|\\text{The Industrial Revolution began in the 18th})$$\n",
        "\n",
        "using our trigram (3-gram) approximation, we now only need the 2 preceding words in this part of corpus:\n",
        "\n",
        "$$\\normalsize P(\\text{century}|\\text{the 18th})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qc6OEYHgjRr",
        "colab_type": "text"
      },
      "source": [
        "### Computing N-gram Probability\n",
        "\n",
        "When working with bigrams (2-grams):\n",
        "\n",
        "$$ \\Large P(w_n | w_{n-1}) = \\frac{C(w_{n-1} w_n)}{\\displaystyle\\sum_w C(w_{n-1} w)} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUiiACRjrWCK",
        "colab_type": "text"
      },
      "source": [
        "## Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTO9LPmsI51n",
        "colab_type": "text"
      },
      "source": [
        "### Vector Semantics\n",
        "\n",
        "\"The idea of vector semantics is to represent a word as a point in some multi-dimensional semantic space. Vector representing words are generally callled embeddings, because the word is embedded in a particular vector space.\"\n",
        "\n",
        "> **Distributional hypothesis**\n",
        "> \n",
        "> Words that occur in similar contexts tend to have similar meanings.\n",
        ">\n",
        "> **Distributional semantics**\n",
        ">\n",
        "> A word's meaning is given by the words that frequently appear close-by.\n",
        "\n",
        "The vector semantics' model instantiates the distributional hypothesis by learning representations of the meaning of words direclty from their distributions in texts. It offers a fine-grained model of meaning that lets us also implement word similarity (and phrase similarity).\n",
        "\n",
        "Vector semantic models are also extremely practical because they can be learned automatically from text without any complex labeling or supervision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcTf8jrDlKt8",
        "colab_type": "text"
      },
      "source": [
        "### word2vec\n",
        "\n",
        "\"Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in that space.\" [wikipedia https://en.wikipedia.org/wiki/Word2vec]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpxFzYbWKl5f",
        "colab_type": "text"
      },
      "source": [
        "#### Skip-gram\n",
        "\n",
        "One of the word2vec models is skip-gram, which predicts the context words $ \\large w_{t+j}$ within a window of fixed size, given a center word $ \\large w_t $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OnM_2h2lcrV",
        "colab_type": "text"
      },
      "source": [
        "![word2vec diagram](https://user-images.githubusercontent.com/114733/75586670-b5606400-5a53-11ea-9df0-648c9a07e5f8.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrfcdMuakSGy",
        "colab_type": "text"
      },
      "source": [
        "We are looking for the word vectors (embeddings) that maximizes the likelihood of our probability distribution:\n",
        "\n",
        "$$ \\Large  L(\\theta) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m \\\\ \\quad j \\neq 0} P(w_{t+j}|w_t; \\theta) $$\n",
        "\n",
        "and we end up minimizing the average negative log likelihood:\n",
        "\n",
        "$$ \\Large  J(\\theta) = - \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m \\\\ \\quad j \\neq 0} \\log P(w_{t+j}|w_t; \\theta) $$\n"
      ]
    }
  ]
}