{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01a_intro_NLP_and_word_embeddings.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/introNLP/blob/master/01a_intro_NLP_and_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTveffOPrERD",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to NLP with Deep Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K0xL4QmfkOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy==2.2.3\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrxR2RE-Rtin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNHVJ8ELRAwt",
        "colab_type": "text"
      },
      "source": [
        "### Part-of-Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FennpZGGR4ny",
        "colab_type": "code",
        "outputId": "9cd54846-e066-4702-fb21-90917c38ff18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "doc = nlp_en(\"Many Japanese children refuse to go to school\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Many ADJ JJ\n",
            "Japanese ADJ JJ\n",
            "children NOUN NNS\n",
            "refuse VERB VBP\n",
            "to PART TO\n",
            "go VERB VB\n",
            "to ADP IN\n",
            "school NOUN NN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQyL798YZ7fG",
        "colab_type": "text"
      },
      "source": [
        "### Sentence Boundary Disambiguation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzZSKEtra8yt",
        "colab_type": "code",
        "outputId": "f429820f-cbe6-454a-bfb2-f07ef256bf77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "doc = nlp_en('Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.')\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal.\n",
            "Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qoqhte2ubyvY",
        "colab_type": "code",
        "outputId": "4e66141d-9beb-4c0b-8a00-4d906fa9517d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "text = (\"Facebook, Inc. is an American social media and technology company\"\n",
        "        \" based in Menlo Park, California.\")\n",
        "\n",
        "doc = nlp_en(text)\n",
        "\n",
        "for key, sent in enumerate(doc.sents):\n",
        "    print(key, sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Facebook, Inc. is an American social media and technology company based in Menlo Park, California.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNPEDf8kap14",
        "colab_type": "text"
      },
      "source": [
        "### SpaCy's Language Support"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auko6O3bay3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hekmT-vEbBzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pt_core_news_sm as language_pt\n",
        "\n",
        "nlp_pt = language_pt.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHMsGUUZbttg",
        "colab_type": "code",
        "outputId": "8f2dd746-e192-4146-efb3-ef140eb4e020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "doc = nlp_pt(\"China anuncia redução de tarifas de importação de mais de 850 produtos\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "China PROPN\n",
            "anuncia VERB\n",
            "redução NOUN\n",
            "de ADP\n",
            "tarifas NOUN\n",
            "de ADP\n",
            "importação NOUN\n",
            "de ADP\n",
            "mais ADV\n",
            "de ADP\n",
            "850 NUM\n",
            "produtos SYM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18CvZLpxUWag",
        "colab_type": "text"
      },
      "source": [
        "## Language Modeling\n",
        "\n",
        "\"Language modeling is the task of assigning a probability to sentences in a language (what is the probability of seeing the sentence *the lazy dog barked loudly*?). Besides assigning a probability to each sequence of words, the language models also assign a probability for the likelihood of a given word (or sequence of words) to follow a sequence of words (what is the probability of seeing the word *barked* after seeing the sequence *the lazy dog*?).\" [1]\n",
        "\n",
        "Formally, the probability of a sequence of words $\\Large w_{1:n}$, ussing the chain rule of probability, is:\n",
        "\n",
        "$$\\Large P(w_{1:n}) = P(w_1)P(w_2|w_1)P(w_3|w_{1:2})...P(w_n|w_{1:n-1})$$\n",
        "\n",
        "which can be written using the product of sequences symbol:\n",
        "\n",
        "$$ \\Large P(w_{1:n}) = \\displaystyle \\prod_{k=1}^{n} P(w_{k} | w_{1:k-1}) $$\n",
        "\n",
        "where $\\Large w_{1:n}$ is a sequence of words: $\\Large w_1 w_2 w_3 ... w_{n-1} w_{n}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaIEe97WuzCI",
        "colab_type": "text"
      },
      "source": [
        "### N-grams\n",
        "\n",
        "N-grams are models that approximate the probability of a word given all the previous words $ P(w_{n} | w_{1:n-1}) $ by only using the conditional probability  of the preceding  $ N - 1 $ words.\n",
        "\n",
        "Bigrams (2-grams)\n",
        "\n",
        "When $N=2$ we have:\n",
        "\n",
        "$$\\Large P(w_n | w_{1:n-1}) = P(w_n | w_{n-1})$$\n",
        "\n",
        "When $N=3$ we have:\n",
        "\n",
        "$$\\Large P(w_n | w_{1:n-1}) = P(w_n | w_{n-2:n-1})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUiiACRjrWCK",
        "colab_type": "text"
      },
      "source": [
        "## Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTO9LPmsI51n",
        "colab_type": "text"
      },
      "source": [
        "### Vector Semantics\n",
        "\n",
        "\"The idea of vector semantics is to represent a word as a point in some multi-dimensional semantic space. Vector representing words are generally callled embeddings, because the word is embedded in a particular vector space.\"\n",
        "\n",
        "> **Distributional hypothesis**\n",
        "> \n",
        "> Words that occur in similar contexts tend to have similar meanings.\n",
        ">\n",
        "> **Distributional semantics**\n",
        ">\n",
        "> A word's meaning is given by the words that frequently appear close-by.\n",
        "\n",
        "The vector semantics' model instantiates the distributional hypothesis by learning representations of the meaning of words direclty from their distributions in texts. It offers a fine-grained model of meaning that lets us also implement word similarity (and phrase similarity).\n",
        "\n",
        "Vector semantic models are also extremely practical because they can be learned automatically from text without any complex labeling or supervision.\n"
      ]
    }
  ]
}